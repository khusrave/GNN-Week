{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This tutorial is adapted from [WikiNet — An Experiment in Recurrent Graph Neural Networks](https://medium.com/stanford-cs224w/wikinet-an-experiment-in-recurrent-graph-neural-networks-3f149676fbf3) by Alexander Hurtado."
      ],
      "metadata": {
        "id": "Weu0WzWVe0nP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WikiNet\n",
        "\n",
        "WikiNet tackles the target prediction problem on the Wikispeedia dataset. Namely, given a sequence of articles clicked by a player, the task is to predict the final target article the user is searching for. The following code is of the model definition, training, and evaluation for the experiments."
      ],
      "metadata": {
        "id": "kEi9i7wEkjIc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we begin by installing the necessary libraries and dataset!"
      ],
      "metadata": {
        "id": "lXXXMdUGp0hR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2fq2m29soI-Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a51a30d-0f53-4541-b848-2ba737a16154"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-1.10.0+cu111.html\n",
            "Collecting torch-scatter\n",
            "  Downloading torch_scatter-2.1.1.tar.gz (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.6/107.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: torch-scatter\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-scatter: filename=torch_scatter-2.1.1-cp310-cp310-linux_x86_64.whl size=3536424 sha256=59baa7a9d2226f6554091f257033cbc9e5a4813c5f229921d443fbb077ad5f76\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/67/58/6566a3b61c6ec0f2ca0c2c324cd035ef2955601f0fb3197d5f\n",
            "Successfully built torch-scatter\n",
            "Installing collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.1\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.10.0+cu111.html\n",
            "Collecting torch-sparse\n",
            "  Downloading torch_sparse-0.6.17.tar.gz (209 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.22.4)\n",
            "Building wheels for collected packages: torch-sparse\n",
            "  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-sparse: filename=torch_sparse-0.6.17-cp310-cp310-linux_x86_64.whl size=2693617 sha256=e846f3000f12628ff40d4c0a792997279dd26928cc23880bad92f56ec9b5916c\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/25/e7/037b58fa47ba781444fd101a2f06c63a9d4e967ca6b910c53a\n",
            "Successfully built torch-sparse\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.17\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.2.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910460 sha256=14edb71e99ee3668b3d208266d1c375afe1e44016fb202b1333a9de5a98b6dc2\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.3.1\n",
            "Collecting class-resolver\n",
            "  Downloading class_resolver-0.4.2-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: class-resolver\n",
            "Successfully installed class-resolver-0.4.2\n",
            "--2023-07-24 07:08:39--  https://github.com/alexanderjhurtado/cs224w_wikinet/raw/main/colab_starter_pack/graph_with_features.gml.zip\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/alexanderjhurtado/cs224w_wikinet/main/colab_starter_pack/graph_with_features.gml.zip [following]\n",
            "--2023-07-24 07:08:39--  https://raw.githubusercontent.com/alexanderjhurtado/cs224w_wikinet/main/colab_starter_pack/graph_with_features.gml.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6705796 (6.4M) [application/zip]\n",
            "Saving to: ‘graph_with_features.gml.zip’\n",
            "\n",
            "graph_with_features 100%[===================>]   6.39M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-07-24 07:08:40 (174 MB/s) - ‘graph_with_features.gml.zip’ saved [6705796/6705796]\n",
            "\n",
            "--2023-07-24 07:08:40--  https://github.com/alexanderjhurtado/cs224w_wikinet/raw/main/colab_starter_pack/paths_and_labels.tsv\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/alexanderjhurtado/cs224w_wikinet/main/colab_starter_pack/paths_and_labels.tsv [following]\n",
            "--2023-07-24 07:08:40--  https://raw.githubusercontent.com/alexanderjhurtado/cs224w_wikinet/main/colab_starter_pack/paths_and_labels.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7353263 (7.0M) [text/plain]\n",
            "Saving to: ‘paths_and_labels.tsv’\n",
            "\n",
            "paths_and_labels.ts 100%[===================>]   7.01M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-07-24 07:08:40 (163 MB/s) - ‘paths_and_labels.tsv’ saved [7353263/7353263]\n",
            "\n",
            "Archive:  /content/graph_with_features.gml.zip\n",
            "  inflating: graph_with_features.gml  \n",
            "  inflating: __MACOSX/._graph_with_features.gml  \n"
          ]
        }
      ],
      "source": [
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n",
        "!pip install torch-geometric\n",
        "!pip install class-resolver\n",
        "\n",
        "!wget --no-cache https://github.com/alexanderjhurtado/cs224w_wikinet/raw/main/colab_starter_pack/graph_with_features.gml.zip\n",
        "!wget --no-cache https://github.com/alexanderjhurtado/cs224w_wikinet/raw/main/colab_starter_pack/paths_and_labels.tsv\n",
        "!unzip -o /content/graph_with_features.gml.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we import all libraries that will be used by the code."
      ],
      "metadata": {
        "id": "ARdVxxwbp43X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zr95n5Vzoqq3"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import time\n",
        "import networkx as nx\n",
        "from torch_geometric.utils import from_networkx\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCN, GAT, GraphSAGE\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the dataset\n",
        "!wget https://github.com/alexanderjhurtado/cs224w_wikinet/blob/main/colab_starter_pack/graph_with_features.gml.zip\n",
        "!wget https://github.com/alexanderjhurtado/cs224w_wikinet/blob/main/colab_starter_pack/paths_and_labels.tsv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9e3vMVvfySQ",
        "outputId": "e46423c4-c966-4d6b-a5f3-6bfb5c1a4d4b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-24 07:08:44--  https://github.com/alexanderjhurtado/cs224w_wikinet/blob/main/colab_starter_pack/graph_with_features.gml.zip\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4904 (4.8K) [text/plain]\n",
            "Saving to: ‘graph_with_features.gml.zip.1’\n",
            "\n",
            "\r          graph_wit   0%[                    ]       0  --.-KB/s               \rgraph_with_features 100%[===================>]   4.79K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-07-24 07:08:44 (66.2 MB/s) - ‘graph_with_features.gml.zip.1’ saved [4904/4904]\n",
            "\n",
            "--2023-07-24 07:08:44--  https://github.com/alexanderjhurtado/cs224w_wikinet/blob/main/colab_starter_pack/paths_and_labels.tsv\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4867 (4.8K) [text/plain]\n",
            "Saving to: ‘paths_and_labels.tsv.1’\n",
            "\n",
            "paths_and_labels.ts 100%[===================>]   4.75K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-07-24 07:08:45 (53.3 MB/s) - ‘paths_and_labels.tsv.1’ saved [4867/4867]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJK41jk8osx9"
      },
      "outputs": [],
      "source": [
        "nx_graph = nx.read_gml('graph_with_features.gml')\n",
        "G = from_networkx(nx_graph, group_node_attrs=['out_degree', 'in_degree', 'category_multi_hot', 'article_embed'])\n",
        "\n",
        "path_data = pd.read_csv('paths_and_labels.tsv', sep='\\t', header=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function will be called during training and evaluation to evaluate the model on the validation and test datasets."
      ],
      "metadata": {
        "id": "mGigWEo0rH2M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xL8mhtOrovBA"
      },
      "outputs": [],
      "source": [
        "def get_evaluation_metrics(model, device, dataloader, dataset_size):\n",
        "    model.eval()\n",
        "    avg_loss = 0\n",
        "    num_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(dataloader):\n",
        "            # get data\n",
        "            inputs = data['indices'].to(device)\n",
        "            labels = data['label'].to(device) #TODO: Get labels\n",
        "            outputs = model(inputs) #TODO: Pass inputs through model to get outputs\n",
        "            # get loss\n",
        "            loss = F.nll_loss(outputs, labels) #TODO: Get loss between outputs and labels using the Negative Log Likelihood Loss\n",
        "            avg_loss += loss.item() #TODO: Get the average loss per one epoch\n",
        "            # get accuracy\n",
        "            pred = outputs.argmax(dim=1)\n",
        "            correct = (pred == labels).sum() #TODO: Find the number of correct predictions\n",
        "            num_correct += correct\n",
        "    acc = int(num_correct) / dataset_size #TODO: Get the accuracy\n",
        "    avg_loss /= dataset_size\n",
        "    return acc, avg_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This defines the dataset class we use to represent the path data."
      ],
      "metadata": {
        "id": "2ZGu5CTXtQPO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tCnsYEvowvO"
      },
      "outputs": [],
      "source": [
        "class CustomPathDataset(Dataset):\n",
        "    def __init__(self, path_data):\n",
        "        self.x = path_data[0].apply(json.loads)\n",
        "        self.labels = path_data[1]\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.LongTensor(self.x[idx])\n",
        "        label = self.labels[idx]\n",
        "        sample = {\"indices\": x, \"label\": label}\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the class definition for the baseline model, an LSTM. Run this cell to be able to train the baseline model."
      ],
      "metadata": {
        "id": "mLISGR7dtTkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Baseline(torch.nn.Module):\n",
        "    def __init__(self, graph, device, node_embed_size=64, lstm_hidden_size=32):\n",
        "        super().__init__()\n",
        "        self.graphX = graph.x.to(device)\n",
        "        self.graphEdgeIndex = graph.edge_index.to(device)\n",
        "        self.lstm_input_size = self.graphX.shape[1]\n",
        "        self.lstm = nn.LSTM(input_size=self.lstm_input_size,\n",
        "                            hidden_size=lstm_hidden_size,\n",
        "                            batch_first=True)\n",
        "        self.pred_head = nn.Linear(lstm_hidden_size, self.graphX.shape[0])\n",
        "\n",
        "    def forward(self, indices):\n",
        "        node_emb = self.graphX\n",
        "        node_emb_with_padding = torch.cat([node_emb, torch.zeros((1, self.lstm_input_size)).to(device)])\n",
        "        paths = node_emb_with_padding[indices]\n",
        "        _, (h_n, _) = self.lstm(paths)\n",
        "        predictions = self.pred_head(torch.squeeze(h_n))\n",
        "        return F.log_softmax(predictions, dim=1)"
      ],
      "metadata": {
        "id": "MSMEyv6Ytcid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the class definition for the Graph Neural Network - based model. GraphSage model is used here as it performed best. If you would like to use GCN or GAT, simply replace `self.gnn = GraphSAGE(...)` with `self.gnn = GCN(...)` or `self.gnn = GAT(...)`, respectively. The arguments are the same for all 3 models.\n",
        "\n",
        "This cell also defines the model weights file. This file will be generated during training, storing the weights for the best model based on validation accuracy during training."
      ],
      "metadata": {
        "id": "ysVSRJp_uGhR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6nU8MS_oyC7"
      },
      "outputs": [],
      "source": [
        "MODEL_WEIGHT_PATH = \"model_weights.pth\"\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, graph, device, sequence_path_length=32, gnn_hidden_size=128, node_embed_size=64, lstm_hidden_size=32):\n",
        "        super().__init__()\n",
        "        self.graphX = graph.x.to(device)\n",
        "        self.graphEdgeIndex = graph.edge_index.to(device)\n",
        "        self.gnn = GraphSAGE(in_channels=self.graphX.shape[1],\n",
        "                       hidden_channels=gnn_hidden_size,\n",
        "                       num_layers=3,\n",
        "                       out_channels=node_embed_size,\n",
        "                       dropout=0.1)\n",
        "        self.batch_norm_lstm = nn.BatchNorm1d(sequence_path_length) #TODO: Applies Batch Normalization (1d) with sequence_path_length as number of input\n",
        "        self.batch_norm_linear = nn.BatchNorm1d(lstm_hidden_size)  #TODO: Applies Batch Normalization (1d) with lstm_hidden_size as number of input\n",
        "        self.lstm_input_size = node_embed_size\n",
        "        self.lstm = nn.LSTM(input_size=self.lstm_input_size,\n",
        "                            hidden_size=lstm_hidden_size,\n",
        "                            batch_first=True) #TODO: Apply a LSTM\n",
        "        self.pred_head = nn.Linear(lstm_hidden_size, self.graphX.shape[0]) #TODO: Initialize the linear layer with the appropriate number of input and output features\n",
        "\n",
        "    def forward(self, indices):\n",
        "        node_emb = self.gnn(self.graphX, self.graphEdgeIndex)\n",
        "        node_emb_with_padding = torch.cat([node_emb, torch.zeros((1, self.lstm_input_size)).to(device)])\n",
        "        paths = node_emb_with_padding[indices]\n",
        "        paths = self.batch_norm_lstm(paths)\n",
        "        _, (h_n, _) = self.lstm(paths)\n",
        "        h_n = self.batch_norm_linear(torch.squeeze(h_n))\n",
        "        predictions = self.pred_head(h_n)\n",
        "        return F.log_softmax(predictions, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we set up the `train / val / test` split as `90 / 5 / 5`. Moreover, we define the hyperparameters, including the learning rate, the optimizer (Adam), and the batch size."
      ],
      "metadata": {
        "id": "8XwpRS2VwnV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the dataset + splits\n",
        "dataset = CustomPathDataset(path_data)  # TODO: Get the dataset using the class we've created above\n",
        "train_size = int(0.9 * len(dataset))    # TODO: Get the train size\n",
        "test_size = int(0.05 * len(dataset))    # TODO: Get the test size\n",
        "val_size = len(dataset) - train_size - test_size # TODO: Get the vaule size\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size]) #TODO: Get the datasets using torch.utils.data.random_split\n",
        "\n",
        "# set up for training + validation\n",
        "batch_size = 1024\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "validloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "XmVP9ZVOED0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline Model"
      ],
      "metadata": {
        "id": "i1bfm4i3-3v9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFAmfEgl-1rm"
      },
      "outputs": [],
      "source": [
        "# set up the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Baseline(G, device).to(device) #TODO: Set up the model using the Baseline Class and send it to device\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01) #TODO: Initialize the optimizer with the appropriate parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the training script. We train the model for 200 epochs and print training loss, validation loss, validation accuracy, and time spent for each epoch.\n",
        "\n",
        "Moreover, we train by running one batch through the model at a time and using the Negative Log Likelihood loss function. We also save the model weights for the best validation accuracy we see after an epoch. These weights will be used in the evaluation step."
      ],
      "metadata": {
        "id": "7qc3D6BX-1rn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjG-xLlf-1rn"
      },
      "outputs": [],
      "source": [
        "best_acc = 0\n",
        "training_losses = []\n",
        "validation_losses = []\n",
        "validation_accs = []\n",
        "model.train()\n",
        "for epoch in range(50):  # loop over the dataset multiple times\n",
        "    print('Epoch:', epoch+1)\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    start_time = time.time()\n",
        "    for i, data in enumerate(trainloader):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs = data['indices'].to(device) #TODO: Get the inputs\n",
        "        labels = data['label'].to(device) #TODO: Get the labels\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad() #TODO: Reset the gradients\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs) #TODO: Pass inputs through model to get outputs\n",
        "        loss = F.nll_loss(outputs, labels) #TODO: Get loss between outputs and labels using the Negative Log Likelihood Loss\n",
        "        epoch_loss += loss.item() #TODO: Get the average loss per one epoch\n",
        "        loss.backward() #TODO: Perform the backward pass\n",
        "        optimizer.step() #TODO: Perform the optimization step\n",
        "\n",
        "    # validate epoch and print results\n",
        "    training_losses.append(epoch_loss / train_size)\n",
        "    print('Training Loss:', training_losses[-1])\n",
        "    acc, valid_loss = get_evaluation_metrics(model, device, validloader, val_size) #TODO: Get accuracy and validation loss using the function get_evaluation_metrics()\n",
        "    validation_losses.append(valid_loss)\n",
        "    validation_accs.append(acc)\n",
        "    if acc > best_acc:\n",
        "        torch.save(model.state_dict(), MODEL_WEIGHT_PATH)\n",
        "        best_acc = acc\n",
        "    print(\"Validation accuracy:\", acc)\n",
        "    print(\"Validation loss:\", valid_loss)\n",
        "    print('Time elapsed:', time.time() - start_time)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code runs evaluation on the test dataset. In particular, it uses the weights from the best validation accuracy to obtain the test accuracy.\n",
        "\n",
        "This cell will print out the \"loss\" and accuracy on the testing dataset."
      ],
      "metadata": {
        "id": "oA4t8Tm9-1ro"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlaQZ5Ks-1ro",
        "outputId": "3b467ab5-a007-4abf-a4cb-19ede208b4ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.27262090483619345\n",
            "Test loss: 0.006432513922871368\n"
          ]
        }
      ],
      "source": [
        "# model.load_state_dict(torch.load(MODEL_WEIGHT_PATH))\n",
        "model.eval() #TODO: Evaluate the model\n",
        "acc, test_loss = get_evaluation_metrics(model, device, testloader, test_size) #TODO: Get accuracy and test loss using the function get_evaluation_metrics()\n",
        "print(\"Test accuracy:\", acc)\n",
        "print(\"Test loss:\", test_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graph Neural Network"
      ],
      "metadata": {
        "id": "EGDj7r4Z-wms"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sR5HTLUgo0lX"
      },
      "outputs": [],
      "source": [
        "# set up the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Model(G, device).to(device) #TODO: Set up the model using the Model Class and send it to device\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01) #TODO: Initialize the optimizer with the appropriate parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the training script. We train the model for 200 epochs and print training loss, validation loss, validation accuracy, and time spent for each epoch.\n",
        "\n",
        "Moreover, we train by running one batch through the model at a time and using the Negative Log Likelihood loss function. We also save the model weights for the best validation accuracy we see after an epoch. These weights will be used in the evaluation step."
      ],
      "metadata": {
        "id": "X4qpQl1YxGUK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mxsa2R8hp9_Q"
      },
      "outputs": [],
      "source": [
        "best_acc = 0\n",
        "training_losses = []\n",
        "validation_losses = []\n",
        "validation_accs = []\n",
        "model.train()\n",
        "for epoch in range(50):  # loop over the dataset multiple times\n",
        "    print('Epoch:', epoch+1)\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    start_time = time.time()\n",
        "    for i, data in enumerate(trainloader):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs = data['indices'].to(device) #TODO: Get the inputs\n",
        "        labels = data['label'].to(device) #TODO: Get the labels\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad() #TODO: Reset the gradients\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs) #TODO: Pass inputs through model to get outputs\n",
        "        loss = F.nll_loss(outputs, labels) #TODO: Get loss between outputs and labels using the Negative Log Likelihood Loss\n",
        "        epoch_loss += loss.item() #TODO: Get the average loss per one epoch\n",
        "        loss.backward()  #TODO: Perform the backward pass\n",
        "        optimizer.step() #TODO: Perform the optimization step\n",
        "\n",
        "    # validate epoch and print results\n",
        "    training_losses.append(epoch_loss / train_size)\n",
        "    print('Training Loss:', training_losses[-1])\n",
        "    acc, valid_loss = get_evaluation_metrics(model, device, validloader, val_size) #TODO: Get accuracy and validation loss using the function get_evaluation_metrics()\n",
        "    validation_losses.append(valid_loss)\n",
        "    validation_accs.append(acc)\n",
        "    if acc > best_acc:\n",
        "        torch.save(model.state_dict(), MODEL_WEIGHT_PATH)\n",
        "        best_acc = acc\n",
        "    print(\"Validation accuracy:\", acc)\n",
        "    print(\"Validation loss:\", valid_loss)\n",
        "    print('Time elapsed:', time.time() - start_time)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code runs evaluation on the test dataset. In particular, it uses the weights from the best validation accuracy to obtain the test accuracy.\n",
        "\n",
        "This cell will print out the \"loss\" and accuracy on the testing dataset."
      ],
      "metadata": {
        "id": "Xig9d5Kxxilv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_sObjK9o5_4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "090dcf06-8956-412b-8963-895f1f11727f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.3654446177847114\n",
            "Test loss: 0.005760757115999362\n"
          ]
        }
      ],
      "source": [
        "# model.load_state_dict(torch.load(MODEL_WEIGHT_PATH))\n",
        "model.eval() #TODO: Evaluate the model\n",
        "acc, test_loss = get_evaluation_metrics(model, device, testloader, test_size) #TODO: Get accuracy and test loss using the function get_evaluation_metrics()\n",
        "print(\"Test accuracy:\", acc)\n",
        "print(\"Test loss:\", test_loss)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}